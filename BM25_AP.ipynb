{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train_cleaned_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b75d81955d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_cleaned_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train_cleaned_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = []\n",
    "for index, row in df.iterrows():\n",
    "    s = str(row['string_remove_stopwords'])\n",
    "    s = s.split()\n",
    "    remove_stopwords.append(s)\n",
    "\n",
    "df[\"remove_stopwords\"] = remove_stopwords\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>string_remove_stopwords</th>\n",
       "      <th>company_lower</th>\n",
       "      <th>occupation_lower</th>\n",
       "      <th>place_lower</th>\n",
       "      <th>keywords_combined</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>lst_keywords_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2502</td>\n",
       "      <td>would create closing game plan would brought d...</td>\n",
       "      <td>uniqlo</td>\n",
       "      <td>sales floor supervisor and visuals merchandiser</td>\n",
       "      <td>5 av and 53 st and times square</td>\n",
       "      <td>would create closing game plan would brought d...</td>\n",
       "      <td>[would, create, closing, game, plan, would, br...</td>\n",
       "      <td>[would, create, closing, game, plan, would, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1605</td>\n",
       "      <td>every day learning day dealing every different...</td>\n",
       "      <td>uniqlo</td>\n",
       "      <td>sale advisor</td>\n",
       "      <td>london, eng</td>\n",
       "      <td>every day learning day dealing every different...</td>\n",
       "      <td>[every, day, learning, day, dealing, every, di...</td>\n",
       "      <td>[every, day, learning, day, dealing, every, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2438</td>\n",
       "      <td>work environment conducive worklife balance ac...</td>\n",
       "      <td>singtel</td>\n",
       "      <td>sales &amp; accounts operations executive</td>\n",
       "      <td>singapore</td>\n",
       "      <td>work environment conducive worklife balance ac...</td>\n",
       "      <td>[work, environment, conducive, worklife, balan...</td>\n",
       "      <td>[work, environment, conducive, worklife, balan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>818</td>\n",
       "      <td>work branch theres much information provide ti...</td>\n",
       "      <td>ocbc bank</td>\n",
       "      <td>personal financial consultant</td>\n",
       "      <td>seremban</td>\n",
       "      <td>work branch theres much information provide ti...</td>\n",
       "      <td>[work, branch, theres, much, information, prov...</td>\n",
       "      <td>[work, branch, theres, much, information, prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1079</td>\n",
       "      <td>open culture openminded bosses supportive coll...</td>\n",
       "      <td>starhub</td>\n",
       "      <td>senior hr partner</td>\n",
       "      <td>uni</td>\n",
       "      <td>open culture openminded bosses supportive coll...</td>\n",
       "      <td>[open, culture, openminded, bosses, supportive...</td>\n",
       "      <td>[open, culture, openminded, bosses, supportive...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                            string_remove_stopwords company_lower  \\\n",
       "0  2502  would create closing game plan would brought d...        uniqlo   \n",
       "1  1605  every day learning day dealing every different...        uniqlo   \n",
       "2  2438  work environment conducive worklife balance ac...       singtel   \n",
       "3   818  work branch theres much information provide ti...     ocbc bank   \n",
       "4  1079  open culture openminded bosses supportive coll...       starhub   \n",
       "\n",
       "                                  occupation_lower  \\\n",
       "0  sales floor supervisor and visuals merchandiser   \n",
       "1                                     sale advisor   \n",
       "2            sales & accounts operations executive   \n",
       "3                    personal financial consultant   \n",
       "4                                senior hr partner   \n",
       "\n",
       "                       place_lower  \\\n",
       "0  5 av and 53 st and times square   \n",
       "1                      london, eng   \n",
       "2                        singapore   \n",
       "3                         seremban   \n",
       "4                              uni   \n",
       "\n",
       "                                   keywords_combined  \\\n",
       "0  would create closing game plan would brought d...   \n",
       "1  every day learning day dealing every different...   \n",
       "2  work environment conducive worklife balance ac...   \n",
       "3  work branch theres much information provide ti...   \n",
       "4  open culture openminded bosses supportive coll...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [would, create, closing, game, plan, would, br...   \n",
       "1  [every, day, learning, day, dealing, every, di...   \n",
       "2  [work, environment, conducive, worklife, balan...   \n",
       "3  [work, branch, theres, much, information, prov...   \n",
       "4  [open, culture, openminded, bosses, supportive...   \n",
       "\n",
       "                               lst_keywords_combined  \n",
       "0  [would, create, closing, game, plan, would, br...  \n",
       "1  [every, day, learning, day, dealing, every, di...  \n",
       "2  [work, environment, conducive, worklife, balan...  \n",
       "3  [work, branch, theres, much, information, prov...  \n",
       "4  [open, culture, openminded, bosses, supportive...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_keywords_combined = []\n",
    "for index, row in df.iterrows():\n",
    "    s = str(row['keywords_combined'])\n",
    "    s = s.split()\n",
    "    lst_keywords_combined.append(s)\n",
    "\n",
    "df[\"lst_keywords_combined\"] = lst_keywords_combined\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_(doc):\n",
    "    frequencies = {} \n",
    "#     print(\"doc\", type(doc))\n",
    "    for term in doc:\n",
    "#         print(\"term\", term)\n",
    "        term_count = frequencies.get(term, 0) + 1\n",
    "        frequencies[term] = term_count \n",
    "    return frequencies\n",
    "\n",
    "# for doc in df['remove_stopwords']:\n",
    "#     print(tf_(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_(docs):\n",
    "    df = {}\n",
    "    for doc in docs:\n",
    "        for term in set(doc):\n",
    "            df_count = df.get(term, 0) + 1 \n",
    "            df[term] = df_count\n",
    "    return df\n",
    "\n",
    "# print(df_(df['remove_stopwords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_(df, corpus_size):\n",
    "    import math\n",
    "    idf = {}\n",
    "    for term, freq in df.items():\n",
    "        idf[term] = round(math.log((corpus_size) / (freq)),2) \n",
    "    return idf\n",
    "\n",
    "# print(idf_(df_(df['remove_stopwords']), len(df['remove_stopwords'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score(query, doc, docs, k1=1.5, b=0.75): \n",
    "    score = 0.0\n",
    "    tf = tf_(doc)\n",
    "    df = df_(docs)\n",
    "    idf = idf_(df, len(docs))\n",
    "    avg_doc_len = sum(len(doc) for doc in docs)/len(docs) \n",
    "    for term in query:\n",
    "        if term not in tf.keys(): \n",
    "            continue\n",
    "        #\n",
    "        numerator = idf[term] * tf[term] * (k1 + 1)\n",
    "        denominator = tf[term] + k1 * (1 - b + b * len(doc) / avg_doc_len) \n",
    "        score += (numerator / denominator)\n",
    "    #\n",
    "#     print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/PRASH/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: is uniqlo a good place to work at?\n",
      "is uniqlo a good place to work at?\n",
      "query:  ['is', 'uniqlo', 'a', 'good', 'place', 'to', 'work', 'at']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "query_input = input(\"Enter query: \").lower()\n",
    "query_no_punctuation = str(query_input).translate(str.maketrans('', '', string.punctuation))\n",
    "query = query_no_punctuation.split()\n",
    "# query = [w for w in query_split if not w.lower() in stop_words]\n",
    "\n",
    "print(query_input)\n",
    "# print(\"query_split: \", query_split)\n",
    "# print(\"query_no_punctuation: \", query_no_punctuation)\n",
    "print(\"query: \", query)\n",
    "\n",
    "# Charles One eazy for the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To compare between reviews and reviews + occupation + location + company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:  ['would', 'create', 'closing', 'game', 'plan', 'would', 'brought', 'date', 'soon', 'clocked', 'making', 'sure', 'full', 'sky', 'floor', 'well', 'walking', 'sf', 'making', 'sure', 'proper', 'color', 'flow', 'making', 'sure', 'room', 'makes', 'sense']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = df['remove_stopwords'][0]\n",
    "print(\"doc: \", doc)\n",
    "\n",
    "docs = df['remove_stopwords']\n",
    "\n",
    "_score(query, doc, docs, k1=1.5, b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:  ['would', 'create', 'closing', 'game', 'plan', 'would', 'brought', 'date', 'soon', 'clocked', 'making', 'sure', 'full', 'sky', 'floor', 'well', 'walking', 'sf', 'making', 'sure', 'proper', 'color', 'flow', 'making', 'sure', 'room', 'makes', 'sense', 'sales', 'floor', 'supervisor', 'and', 'visuals', 'merchandiser', 'uniqlo', '5', 'av', 'and', '53', 'st', 'and', 'times', 'square']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.692611668431348"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = df['lst_keywords_combined'][0]\n",
    "print(\"doc: \", doc)\n",
    "\n",
    "docs = df['lst_keywords_combined']\n",
    "\n",
    "_score(query, doc, docs, k1=1.5, b=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "docs = df['lst_keywords_combined']\n",
    "score = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(len(docs)):\n",
    "    doc = docs[i]\n",
    "#     print(doc)\n",
    "    bm25 = _score(query, doc, docs, k1=1.5, b=0.75)\n",
    "    score.append(bm25)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score:  10.393648548755886\n",
      "Time taken: 244.4947850704193s\n",
      "index 680: singtel, to handle project team members by assisting, serangoon north\n"
     ]
    }
   ],
   "source": [
    "m = max(score)\n",
    "time_taken = end_time - start_time\n",
    "idx = score.index(max(score))\n",
    "\n",
    "print(\"Max Score: \", m)\n",
    "print(f\"Time taken: {time_taken}s\")\n",
    "print('index ' + str(idx) + ': ' + df['company_lower'][idx] + ', ' + df['occupation_lower'][idx] + ', ' + df['place_lower'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about top 5 results instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rank Number --> 1\n",
      "Document is --> ['handle', 'project', 'team', 'members', 'assisting', 'paper', 'work', 'filing', 'taking', 'minutes', 'meeting', 'work', 'site', 'office', 'to', 'handle', 'project', 'team', 'members', 'by', 'assisting', 'singtel', 'serangoon', 'north']\n",
      "Cosine Similarity score is --> 10.393648548755886\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 2\n",
      "Document is --> ['friendly', 'cooperative', 'colleagues', 'good', 'teamwork', 'challenging', 'tasks', 'tight', 'datelines', 'end', 'month', 'supportive', 'understanding', 'boss', 'flexible', 'lunch', 'hours', 'assist', 'at', 'team', 'bonding', 'singtel', 'pasir', 'ris', 'exchange', 'and', 'jurong', 'east', 'exchange']\n",
      "Cosine Similarity score is --> 9.84464589479124\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 3\n",
      "Document is --> ['routine', 'day', 'day', 'work', 'monitoring', 'troubleshooting', 'issues', 'projects', 'run', 'specific', 'date', 'lines', 'working', 'good', 'pool', 'cooperative', 'team', 'good', 'team', 'work', 'challenging', 'task', 'meet', 'user', 'expectation', 'high', 'expectation', 'main', 'concerned', 'enjoyable', 'successfully', 'closure', 'one', 'task', 'project', 'shows', 'job', 'satisfaction', 'challenges', 'finally', 'meet', 'success', 'end', 'is', 'lead', 'engineer', 'starhub', '67', 'ubi', 'ave', '1']\n",
      "Cosine Similarity score is --> 9.107325986599363\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 4\n",
      "Document is --> ['fun', 'company', 'work', 'multi', 'racial', 'definitely', 'get', 'chance', 'proof', 'oneself', 'unfortunately', 'leave', 'enable', 'upgrade', 'overseas', 'nice', 'working', 'environment', 'people', 'friendly', 'secretary', 'for', 'a', 'period', 'of', '1', 'year', 'ocbc', 'bank', 'head', 'office']\n",
      "Cosine Similarity score is --> 8.338685703519166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 5\n",
      "Document is --> ['good', 'place', 'work', 'enjoy', 'change', 'place', 'sales', 'associate/cashier/customer', 'service', 'uniqlo', 'soho,', 'ny']\n",
      "Cosine Similarity score is --> 7.9297070376444445\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 6\n",
      "Document is --> ['fun', 'place', 'work', 'coworkers', 'learn', 'new', 'skills', 'fold', 'tidy', 'clothes', 'depending', 'store', 'management', 'good', 'bad', 'good', 'place', 'work', 'sales', 'associate', 'and', 'cashier', 'uniqlo', 'toronto,', 'on']\n",
      "Cosine Similarity score is --> 7.54293445051765\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 7\n",
      "Document is --> ['uniqlo', 'good', 'place', 'work', 'develop', 'lot', 'person', 'talents', 'work', 'environment', 'good', 'work', 'team', 'good', 'part', 'management', 'slight', 'disorganization', 'schedules', 'means', 'know', 'schedules', 'following', 'week', 'customer', 'service', 'advisor', 'uniqlo', 'london']\n",
      "Cosine Similarity score is --> 7.1909246959074755\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 8\n",
      "Document is --> ['uniqlo', 'good', 'place', 'work', 'beware', 'competitive', 'hard', 'working', 'looking', 'easy', 'job', 'uniqlo', 'hr', 'coordinator', 'uniqlo', 'new', 'jersey']\n",
      "Cosine Similarity score is --> 6.985865743104868\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 9\n",
      "Document is --> ['overall', 'good', 'place', 'start', 'retail', 'career', 'work', 'good', 'standards', 'lot', 'initiatives', 'growing', 'company', 'management', 'seeks', 'work', 'sales', 'consultant', 'uniqlo', 'new', 'york,', 'ny']\n",
      "Cosine Similarity score is --> 6.928677858492777\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank Number --> 10\n",
      "Document is --> ['good', 'place', 'work', 'fun', 'work', 'recommend', 'friend', 'work', 'dbs', 'good', 'place', 'work', 'moreover', 'competitive', 'market', 'help', 'us', 'develop', 'knowledge', 'dbs', 'officer', 'dbs', 'bank', 'chennai,', 'tamil', 'nadu']\n",
      "Cosine Similarity score is --> 6.907447804104579\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_dict = {k:v for k, v in enumerate(score)}\n",
    "# print(score_dict)\n",
    "\n",
    "sorted_score_dict = {k: v for k, v in sorted(score_dict.items(), key=lambda item: item[1], reverse = True)}\n",
    "# print(sorted_score_dict)\n",
    "\n",
    "\n",
    "rank = 0\n",
    "for k,v in sorted_score_dict.items():\n",
    "    rank = rank + 1\n",
    "    print(\"\\n\")\n",
    "    print(f\"Rank Number --> {rank}\")\n",
    "    print(f\"Document is --> {df['lst_keywords_combined'][k]}\")\n",
    "    print(f\"Cosine Similarity score is --> {v}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #top ten\n",
    "    if rank == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant Documents\n",
    "\n",
    "#BM25\n",
    "\n",
    "#Query 1: is uniqlo a good place to work at?\n",
    "\n",
    "# Model 1:  5,6,7,8,9\n",
    "# Model 2:  3,6,7,8,9\n",
    "\n",
    "\n",
    "\n",
    "#Query 2: does dbs pay its employees well?\n",
    "\n",
    "#Model 1: 1,3,5,8\n",
    "#Model 2:  2,5,9\n",
    "\n",
    "\n",
    "\n",
    "#Query 3: Does singtel have a nice working environment?\n",
    "\n",
    "#Model 1: 2,3,5,6,7,8,9,10\n",
    "#Model 2: nil\n",
    "\n",
    "\n",
    "\n",
    "#Query 4: What kind of Jobs are there in kuala lumpur?\n",
    "\n",
    "#Model 1: 1,2,3,4,5,7,8,9,10\n",
    "#Model 2: nil\n",
    "\n",
    "\n",
    "\n",
    "#Query 5: Best place to work as a manager\n",
    "\n",
    "#Model 1: 3,5,8,10\n",
    "#Model 2: nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c0c3d8612a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_score_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bm25_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(top_results['Index'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_company\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    420\u001b[0m                                          dtype=values.dtype, copy=False)\n\u001b[1;32m    421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "top_results = pd.DataFrame(sorted_score_dict.items(), columns=['Index', 'bm25_results'])\n",
    "\n",
    "# print(top_results['Index'])\n",
    "\n",
    "result_company = []\n",
    "result_occupation = []\n",
    "result_place = []\n",
    "result_keywords = []\n",
    "\n",
    "for i in range(len(top_results['Index'])):\n",
    "    result_company.append(df['company_lower'][top_results['Index'][i]])\n",
    "    result_occupation.append(df['occupation_lower'][top_results['Index'][i]])\n",
    "    result_place.append(df['place_lower'][top_results['Index'][i]])\n",
    "    result_keywords.append(df['keywords_combined'][top_results['Index'][i]])\n",
    "    \n",
    "top_results['Company'] = result_company\n",
    "top_results['Occupation'] = result_occupation\n",
    "top_results['Place'] = result_place\n",
    "top_results['Keywords'] = result_keywords\n",
    "\n",
    "top_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066666666666667"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MEAN RECIPROCAL RANK  (MODEL 1)\n",
    "import numpy as np\n",
    "\n",
    "def model_1_mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "#     query_1_score = [0,0,0,0,1,1,1,1,1,0]\n",
    "#     query_2_score = [1,0,1,0,1,0,0,1,0,0]\n",
    "#     query_3_score = [0,1,1,0,1,1,1,1,1,1]\n",
    "#     query_4_score = [1,1,1,1,1,0,1,1,1,1]\n",
    "#     query_5_score = [0,0,1,0,1,0,0,1,0,1]\n",
    "   \n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "rs = [[0,0,0,0,1,1,1,1,1,0],[1,0,1,0,1,0,0,1,0,0],[0,1,1,0,1,1,1,1,1,1],[1,1,1,1,1,0,1,1,1,1],[0,0,1,0,1,0,0,1,0,1]]\n",
    "model_1_mean_reciprocal_rank(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MEAN RECIPROCAL RANK  (MODEL 2)\n",
    "import numpy\n",
    "\n",
    "def model_2_mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "#     query_1_score = [0,0,1,0,0,1,1,1,1,0]\n",
    "#     query_2_score = [0,1,0,0,1,0,0,0,1,0]\n",
    "#     query_3_score = [0,0,0,0,0,0,0,0,0,0]\n",
    "#     query_4_score = [0,0,0,0,0,0,0,0,0,0]\n",
    "#     query_5_score = [0,0,0,0,0,0,0,0,0,0]\n",
    "   \n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "rs = [[0,0,1,0,0,1,1,1,1,0],[0,1,0,0,1,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]\n",
    "model_2_mean_reciprocal_rank(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1 IS BETTER, SCORE IS CLOSER TO 1 --> FIRST SEARCH RESULT (DOCUMENT) MORE LIKELY TO BE RELEVANT THAN THAT OF MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE PRECISION AND MEAN AVERAGE PRECISION (MODEL 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40349206349206346"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_average_precision_query1(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "model1_average_precision_query1([0,0,0,0,1,1,1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6916666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_average_precision_query2(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model1_average_precision_query2([1,0,1,0,1,0,0,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6844246031746032"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_average_precision_query3(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model1_average_precision_query3([0,1,1,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9467813051146384"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_average_precision_query4(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model1_average_precision_query4([1,1,1,1,1,0,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3770833333333333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_average_precision_query5(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model1_average_precision_query5([0,0,1,0,1,0,0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6206895943562609"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model1_mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "rs = [[0,0,0,0,1,1,1,1,1,0],[1,0,1,0,1,0,0,1,0,0],[0,1,1,0,1,1,1,1,1,1],[1,1,1,1,1,0,1,1,1,1],[0,0,1,0,1,0,0,1,0,1]]\n",
    "model1_mean_average_precision(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE PRECISION AND MEAN AVERAGE PRECISION (MODEL 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43015873015873013"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_average_precision_query1(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "model2_average_precision_query1([0,0,1,0,0,1,1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41111111111111115"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_average_precision_query2(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model2_average_precision_query2([0,1,0,0,1,0,0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_average_precision_query3(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model2_average_precision_query3([0,0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_average_precision_query4(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model2_average_precision_query4([0,0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_average_precision_query5(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "model2_average_precision_query5([0,0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16825396825396827"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model2_mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "rs = [[0,0,1,0,0,1,1,1,1,0],[0,1,0,0,1,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]\n",
    "model2_mean_average_precision(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
